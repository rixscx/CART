# ğŸŒ³ CART Algorithm (Classification and Regression Trees)

This repository provides a comprehensive overview of the **CART algorithm**, one of the most popular and foundational decision tree algorithms in machine learning. CART is used for both **classification** and **regression** tasks and builds binary trees using the **Gini Index** or **Mean Squared Error** as the splitting criterion.

---

## ğŸ“˜ What is CART?

**CART** stands for **Classification and Regression Trees**. It is a **supervised learning algorithm** that creates binary decision trees for:

- **Classification tasks** (predicting categories)
- **Regression tasks** (predicting continuous values)

CART forms the basis for many advanced ensemble methods such as **Random Forests** and **Gradient Boosting Trees**.

---

## ğŸ” Key Features

| Feature                | Description                                               |
|------------------------|-----------------------------------------------------------|
| ğŸ§  Learning Type        | Supervised (Classification and Regression)               |
| ğŸŒ² Tree Type            | Binary Tree (each node splits into two branches)         |
| ğŸ“ˆ Splitting Criteria   | - Gini Index (for classification) <br> - MSE (for regression) |
| ğŸ§® Handles Continuous?  | âœ… Yes                                                    |
| â“ Handles Missing Data | âš ï¸ Not natively (preprocessing required)                  |
| âœ‚ï¸ Pruning              | âœ… Yes â€“ Supports pre-pruning and post-pruning            |

---

## âš™ï¸ How CART Works

### For Classification:
1. At each node, CART evaluates all possible splits.
2. Uses the **Gini Index** to determine the best feature and threshold.
3. Builds a binary tree recursively.
4. Optionally prunes the tree to avoid overfitting.

### For Regression:
1. Uses **Mean Squared Error (MSE)** to split data.
2. Each leaf node predicts the average value of the target in that group.

---

## ğŸ§ª Gini Index Formula (for Classification)

\[
Gini = 1 - \sum_{i=1}^{C} p_i^2
\]

Where \( p_i \) is the probability of class \( i \) at a node. A lower Gini score indicates a better split.

---

## ğŸ›  Real-World Applications

- ğŸ¥ Disease diagnosis (classification)
- ğŸ“‰ Stock price prediction (regression)
- ğŸ’³ Credit scoring (classification)
- ğŸ  House price estimation (regression)
- ğŸ” Fraud detection (classification)

---

## ğŸ†š CART vs Other Tree Algorithms

| Feature               | ID3           | C4.5                | CART                     |
|-----------------------|---------------|---------------------|--------------------------|
| Splitting Criterion   | Info Gain     | Gain Ratio          | Gini (Classification) / MSE (Regression) |
| Handles Continuous?   | âŒ No         | âœ… Yes              | âœ… Yes                   |
| Handles Missing?      | âŒ No         | âœ… Yes              | âš ï¸ Not natively          |
| Pruning               | âŒ No         | âœ… Post-pruning     | âœ… Pre/Post-pruning      |
| Tree Structure        | Multi-way     | Multi-way           | **Binary Tree**          |

---

## ğŸ§° Tools & Implementations

- **Scikit-learn**: Use `DecisionTreeClassifier` or `DecisionTreeRegressor`.
- **R**: Use `rpart` package.
- **XGBoost / LightGBM**: Built on principles from CART.
- **Weka**: Provides a tree builder similar to CART.

---

## ğŸ Example (Python with Scikit-learn)

```python
from sklearn.tree import DecisionTreeClassifier

clf = DecisionTreeClassifier(criterion='gini', max_depth=3)
clf.fit(X_train, y_train)
predictions = clf.predict(X_test)
